{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tensorflow import keras as k\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "from matplotlib import pyplot\n",
    "from tqdm.keras import TqdmCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Basic implementation (see He (2015) p.7)\n",
    "##\n",
    "#output size 32*32 16*16 8*8\n",
    "#layers      1+2n  2n    2n\n",
    "#filters     16    32    64\n",
    "#We use n=3 for now\n",
    "\n",
    "def define_resnet(wdecay):\n",
    "    inputs = k.Input(shape=(32, 32, 3))\n",
    "    layer1 = k.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(inputs)\n",
    "    layer1 = k.layers.BatchNormalization()(layer1)\n",
    "    \n",
    "    #FIRST STACK\n",
    "    x = k.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(layer1)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    x = k.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(x)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    #Adding skip connection to output of residual block\n",
    "    res = k.layers.Activation(k.activations.relu)(k.layers.Add()([layer1, x]))\n",
    "    x = k.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(res)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    x = k.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(x)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    res = k.layers.Activation(k.activations.relu)(k.layers.Add()([res, x]))\n",
    "    x = k.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(res)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    x = k.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(x)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    res = k.layers.Activation(k.activations.relu)(k.layers.Add()([res, x]))\n",
    "    #SECOND STACK\n",
    "    #Subsampling with stride 2\n",
    "    x = k.layers.Conv2D(32, (3, 3), strides=(2,2), activation='relu', kernel_initializer='he_uniform',\n",
    "        padding='same', kernel_regularizer=k.regularizers.l2(wdecay))(res)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    x = k.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(x)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    res = k.layers.Conv2D(32, (1, 1), strides=(2,2), kernel_initializer='he_uniform', padding='same')(res)\n",
    "    res = k.layers.Activation(k.activations.relu)(k.layers.Add()([res, x]))\n",
    "    x = k.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(res)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    x = k.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(x)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    res = k.layers.Activation(k.activations.relu)(k.layers.Add()([res, x]))\n",
    "    x = k.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(res)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    x = k.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(x)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    res = k.layers.Activation(k.activations.relu)(k.layers.Add()([res, x]))\n",
    "    #THIRD STACK\n",
    "    x = k.layers.Conv2D(64, (3, 3), strides=(2,2), activation='relu', kernel_initializer='he_uniform',\n",
    "        padding='same', kernel_regularizer=k.regularizers.l2(wdecay))(res)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    x = k.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(x)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    res = k.layers.Conv2D(64, (1, 1), strides=(2,2), kernel_initializer='he_uniform', padding='same')(res)\n",
    "    res = k.layers.Activation(k.activations.relu)(k.layers.Add()([res, x]))\n",
    "    x = k.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(res)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    x = k.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(x)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    res = k.layers.Activation(k.activations.relu)(k.layers.Add()([res, x]))\n",
    "    x = k.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(res)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    x = k.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
    "        kernel_regularizer=k.regularizers.l2(wdecay))(x)\n",
    "    x = k.layers.BatchNormalization()(x)\n",
    "    res = k.layers.Activation(k.activations.relu)(k.layers.Add()([res, x]))\n",
    "    \n",
    "    outputs = k.layers.Dense(10, activation='softmax')(k.layers.Flatten()(res))\n",
    "\n",
    "    return k.Model(inputs=inputs, outputs=outputs, name='ResNet_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = define_resnet(0)\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diagnostics(history):\n",
    " # plot loss\n",
    " pyplot.subplot(211)\n",
    " pyplot.title('Cross Entropy Loss')\n",
    " pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    " pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    " # plot accuracy\n",
    " pyplot.subplot(212)\n",
    " pyplot.title('Classification Accuracy')\n",
    " pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    " pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    " # save plot to file\n",
    " filename = sys.argv[0].split('/')[-1]\n",
    " pyplot.tight_layout()\n",
    " pyplot.savefig(filename + '_plot.png')\n",
    " pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(ep=180, learning_rate=0.1, wdecay=0):\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels_mean(trainX, testX)\n",
    "    # define model\n",
    "    model = define_resnet(wdecay)\n",
    "    opt = k.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss=k.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    lr_sched = k.callbacks.LearningRateScheduler(resnet_scheduler)\n",
    "    history = model.fit(trainX, trainY, epochs=ep, batch_size=128, validation_data=(testX, testY), verbose=0, callbacks=TqdmCallback(verbose=1))\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=0)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
